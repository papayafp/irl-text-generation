{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsKFg7hOWAoz"
      },
      "source": [
        "%%bash\r\n",
        "\r\n",
        "rm -rf rl-project/\r\n",
        "git clone https://github.com/cailinw/rl-project.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHCCuMbdWDZs"
      },
      "source": [
        "%%bash\r\n",
        "cd rl-project/\r\n",
        "git pull\r\n",
        "cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxAHp714WDdQ"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlQ7OnBQWDg0"
      },
      "source": [
        "%%bash\r\n",
        "pip install transformers\r\n",
        "pip install wandb\r\n",
        "import wandb\r\n",
        "wandb.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB4YQMmNWGo0"
      },
      "source": [
        "import time\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "import torch\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from IPython import display\r\n",
        "\r\n",
        "import sys\r\n",
        "sys.path.append(\"rl-project/imagecoco/\")\r\n",
        "from coco_dataset import COCOImageCaptionsDataset\r\n",
        "from generator import Generator\r\n",
        "from rewarder import Rewarder\r\n",
        "from utils import save"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXO2RnAeWGyq"
      },
      "source": [
        "#########################################################################################\r\n",
        "#  Hyper-parameters\r\n",
        "#########################################################################################\r\n",
        "# Constants\r\n",
        "vocab_size = 4839\r\n",
        "\r\n",
        "# General parameters\r\n",
        "SEQ_LENGTH = 32\r\n",
        "\r\n",
        "# Model parameters\r\n",
        "g_hidden_state_size = 768\r\n",
        "r_hidden_state_size = 128\r\n",
        "action_embed_dim = 32\r\n",
        "\r\n",
        "\r\n",
        "# Generator training step parameters\r\n",
        "generator_batch_size = 32\r\n",
        "roll_num = 4\r\n",
        "\r\n",
        "# Rewarder training step parameters\r\n",
        "real_batch_size = 32\r\n",
        "generated_batch_size = 32\r\n",
        "\r\n",
        "# Training parameters\r\n",
        "G_LEARNING_RATE = 5e-5\r\n",
        "R_LEARNING_RATE = 0.001\r\n",
        "G_CLIP_MAX_NORM = 1.0\r\n",
        "R_CLIP_MAX_NORM = 1.0\r\n",
        "R_MOMENTUM = 0.9\r\n",
        "NUM_ITERS = 100\r\n",
        "G_ITERS = 1\r\n",
        "R_ITERS = 5\r\n",
        "PRETRAIN_ITERS = 120\r\n",
        "restore = False\r\n",
        "\r\n",
        "save_dir = \"/content/gdrive/My Drive/rl-project/checkpoints6\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cWjWh0rWG9Y"
      },
      "source": [
        "#########################################################################################\r\n",
        "#  Initialization and Pretraining\r\n",
        "#########################################################################################\r\n",
        "\r\n",
        "str_map = pickle.load(open(\"rl-project/imagecoco/save/str_map.pkl\", \"rb\"))\r\n",
        "\r\n",
        "# Load models\r\n",
        "generator = Generator(SEQ_LENGTH, str_map, G_CLIP_MAX_NORM)\r\n",
        "rewarder = Rewarder(\r\n",
        "    SEQ_LENGTH,\r\n",
        "    vocab_size,\r\n",
        "    g_hidden_state_size,\r\n",
        "    action_embed_dim,\r\n",
        "    r_hidden_state_size,\r\n",
        "    R_LEARNING_RATE,\r\n",
        "    R_CLIP_MAX_NORM,\r\n",
        "    R_MOMENTUM\r\n",
        ")\r\n",
        "if restore:\r\n",
        "    # Replace this with the path to the model you want to restore\r\n",
        "    print(\"Restored models\")\r\n",
        "    generator.restore_model(\"/content/gdrive/My Drive/rl-project/checkpoints5/generator_60_-50228096.0.pt\")\r\n",
        "    rewarder.restore_model(\"/content/gdrive/My Drive/rl-project/checkpoints5/rewarder_60_-415776.45625.pt\")\r\n",
        "\r\n",
        "# Load training data\r\n",
        "train_data = COCOImageCaptionsDataset(\"rl-project/imagecoco/save/train_data.pkl\")\r\n",
        "train_dataloader = DataLoader(train_data, batch_size=real_batch_size, shuffle=True)\r\n",
        "\r\n",
        "# Pretrain generator\r\n",
        "print(\"Pretraining generator\")\r\n",
        "pretrain_losses = []\r\n",
        "for it in range(PRETRAIN_ITERS):\r\n",
        "    batch_data = next(iter(train_dataloader))\r\n",
        "    loss = generator.pretrain_step(batch_data).data.cpu().numpy()\r\n",
        "    pretrain_losses.append(loss)\r\n",
        "\r\n",
        "\r\n",
        "plt.plot(np.arange(len(pretrain_losses)), np.array(pretrain_losses))\r\n",
        "plt.show()\r\n",
        "\r\n",
        "g_losses = []\r\n",
        "r_losses = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3fIuG50WKt5"
      },
      "source": [
        "#########################################################################################\r\n",
        "#  Main Training Loop\r\n",
        "#########################################################################################\r\n",
        "\r\n",
        "\r\n",
        "for it in range(NUM_ITERS):\r\n",
        "\r\n",
        "    # TRAIN GENERATOR\r\n",
        "    start = time.time()\r\n",
        "    loss_sum = 0\r\n",
        "    for g_it in range(G_ITERS):\r\n",
        "        g_loss = generator.rl_train_step(\r\n",
        "            rewarder, generator_batch_size\r\n",
        "        )\r\n",
        "        loss_sum += g_loss\r\n",
        "    speed = time.time() - start\r\n",
        "    g_losses.append(loss_sum / G_ITERS)\r\n",
        "    print(\r\n",
        "        \"MaxentPolicy Gradient {} iteration, Speed:{:.3f}, Loss:{:.3f}\".format(\r\n",
        "            it, speed, g_loss\r\n",
        "        )\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "    # TRAIN REWARDER\r\n",
        "    start = time.time()\r\n",
        "    loss_sum = 0\r\n",
        "    for r_it in range(R_ITERS):\r\n",
        "        real_trajectories = next(iter(train_dataloader))\r\n",
        "        r_loss = rewarder.train_step(real_trajectories[0], generator, generated_batch_size)\r\n",
        "        loss_sum += r_loss\r\n",
        "    speed = time.time() - start\r\n",
        "    r_losses.append(loss_sum / R_ITERS)\r\n",
        "    print(\r\n",
        "        \"Reward training {} iteration, Speed:{:.3f}, Loss:{:.3f}\".format(\r\n",
        "            it, speed, r_loss\r\n",
        "        )\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "    # Logging\r\n",
        "    if it % 5 == 0 or it == NUM_ITERS - 1 or it == 1:\r\n",
        "        # Save models\r\n",
        "        torch.save(generator.model.state_dict(), f\"{save_dir}/generator_{it}_{g_losses[-1]}.pt\")\r\n",
        "        torch.save(rewarder.model.state_dict(), f\"{save_dir}/rewarder_{it}_{r_losses[-1]}.pt\")\r\n",
        "\r\n",
        "        # Generate samples\r\n",
        "        generated_samples = generator.generate(generator_batch_size, 1, None, False, False, True)\r\n",
        "        output_file = f\"{save_dir}/generator_sample_{it}.txt\"\r\n",
        "        with open(output_file, 'w+') as fout:\r\n",
        "            for sentence in generated_samples[0]:\r\n",
        "                buffer = ' '.join(sentence) + \"\\n\"\r\n",
        "                fout.write(buffer)\r\n",
        "\r\n",
        "        # Plot loss\r\n",
        "        display.clear_output(wait=True)\r\n",
        "        fig, ax = plt.subplots(1,2,figsize=(14,7))\r\n",
        "        ax[0].cla(); ax[0].plot(g_losses)\r\n",
        "        ax[1].cla(); ax[1].plot(r_losses)\r\n",
        "        display.display(plt.gcf())\r\n",
        "        print(it, g_losses, r_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9AiC6aLWKw8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k55gMnjXWK04"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzP8uAoMWK4n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOiOZn8YWHHt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}